{"pages":[],"posts":[{"title":"Bigdata learning","text":"菜鸡的学习过程点滴记录1. Hadoop环境搭建1.1 安装虚拟机以及Centos操作系统我的组合是VMware16、Centos8和Hadoop2.7.7。建议Centos系统是1个graph模式和2个multi模式。 图形界面和命令行切换： 1234systemctl get-default # 查看目前的启动方式 multi-user是命令行模式，graphical是图形界面模式systemctl set-default graphical.target # 设置为图形界面systemctl set-default multi-user.target # 设置为命令行界面reboot # 重启 如果开始安装的是命令行界面，则还需要安装gnome图形界面。 1234yum groupinstall -y &quot;Server with GUI&quot; # 安装图形界面软件包systemctl set-default graphical.targetsystemctl enable gdm --now # 设置图形界面开机自启动reboot # 重启 Centos8右键菜单默认没有打开终端选项，我们可以设置快捷键来启动它 1234567设置 -&gt; 设备 -&gt; display -&gt; keyboard -&gt; 底部添加选项 -&gt; 点击名称: terminal命令: /usr/bin/gnome-terminal快捷键: 自己喜欢的组合键即可，我的是Ctrl+Alt+T 1.2 Hadoop的搭建参考 Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0(2.7.1)/Ubuntu14.04(16.04) Hadoop集群安装配置教程_Hadoop2.6.0_Ubuntu/CentOS 一般的步骤是 关闭并禁用防火墙 创建hadoop用户 更新yum 安装java环境 安装hadoop并配置 克隆虚拟机 配置网络 修改slave的配置文件 配置ssh免密登录 在master上开启Hadoop 过程中最容易出现的错误是最后两步，如果ssh配置后不行的话，可以试参考 ssh配置authorized_keys后仍然需要输入密码的问题 不过具体问题还得看日志，根据日志来解决问题。 2. 编译运行WordCount程序参考 使用命令行编译打包运行自己的MapReduce程序 Hadoop2.6.0 使用Eclipse编译运行MapReduce程序_Hadoop2.6.0_Ubuntu/CentOS 其中eclipse在Centos下的安装程序链接失效，截止2020.10.04，http://eclipse.bluemix.net/packages/2020-09/data/eclipse-java-2020-09-R-linux-gtk-x86_64.tar.gz 还可以下载。 过程中遇到的问题 Input path does not exist: hdfs://master:9000/user/hadoop/input 123hdfs dfs -mkdir -p /user/hadoophdfs dfs -mkdir input/usr/local/hadoop/bin/hadoop fs -put ./input input There are 0 datanode(s) running and no node(s) are excluded in this operation. 1首先见检查三台机器的网络连接，没有问题的话执行$HADOOP_HOME/sbin/stop-all.sh，关闭Hadoop之后重新启动。 Warn: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 123在~/.bashrc下加入export JAVA_LIBRAY_PATH=/usr/local/hadoop/lib/native并source ~/.bashrc即可 运行eclipse提示java版本必须在11及以上 123yum安装jdk11alternatives的使用.bashrc的配置 参考 Linux下怎么切换使用两个版本的JDK Failed: java.net.SocketException: Network is unreachable 检查Hadoop集群的网络连接","link":"/2020/10/05/Bigdata-learning/"}],"tags":[{"name":"Bigdata","slug":"Bigdata","link":"/tags/Bigdata/"}],"categories":[]}